{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "IPhOrEZC14bP"
      },
      "source": [
        "## Implementation\n",
        "\n",
        "In this project, you will need to use information measures to answer several questions. Therefore, in this first part, you are asked to write several functions that implement some of the main measures seen in the first theoretical lectures. Remember that you need to fill in this Jupyter Notebook to answer these questions. Pay particular attention to the required output format of each function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "1ZGhdEwn14bP"
      },
      "outputs": [],
      "source": [
        "# [Locked Cell] You can not import any extra Python library in this Notebook.\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "sbddNKfu14bM"
      },
      "source": [
        "# Project 1 - Information measures\n",
        "\n",
        "The goal of this first project is to get accustomed to the information and uncertainty measures. We ask you to write a brief report (pdf format) collecting your answers to the different questions. All codes must be written in Python inside this Jupyter Notebook. No other code file will be accepted. Note that you can not change the content of locked cells or import any extra Python library than the ones already imported (numpy and pandas)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "JFiTJaFG14bQ"
      },
      "source": [
        "### Question 1\n",
        "\n",
        "Write a function entropy that computes the entropy $\\mathcal{H(X)}$ of a random variable $\\mathcal{X}$ from its probability distribution $P_\\mathcal{X} = (p_1, p_2, . . . , p_n)$. Give the mathematical formula that you are using and explain the key parts of your implementation. Intuitively, what is measured by the entropy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_4WYMYKx14bR"
      },
      "outputs": [],
      "source": [
        "def entropy(Px):\n",
        "    \"\"\"\n",
        "    Computes the entropy from the marginal probability distribution.\n",
        "    Arguments:\n",
        "    ----------\n",
        "    - Px :  Marginal probability distribution of the random\n",
        "            variable X in a numpy array where Px[i]=P(X=i)\n",
        "    Return:\n",
        "    -------\n",
        "    - The entropy of X (H(X)) as a number (integer, float or double).\n",
        "    \"\"\"\n",
        "    return -np.sum(Px * np.log2(Px), where=(Px > 0)) # H(X) = -sum(P(Xi) * log2(P(Xi)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "LJ2Q0y5a14bR"
      },
      "source": [
        "### Question 2\n",
        "\n",
        "Write a function joint_entropy that computes the joint entropy $\\mathcal{H(X,Y)}$ of two discrete random variables $\\mathcal{X}$ and $\\mathcal{Y}$ from the joint probability distribution $P_\\mathcal{X,Y}$. Give the mathematical formula that you are using and explain the key parts of your implementation. Compare the entropy and joint_entropy functions (and their corresponding formulas), what do you notice?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qqK4N-_q14bS"
      },
      "outputs": [],
      "source": [
        "def joint_entropy(Pxy):\n",
        "    \"\"\"\n",
        "    Computes the joint entropy from the joint probability distribution.\n",
        "    Arguments:\n",
        "    ----------\n",
        "    - Pxy:  joint probability distribution of X and Y\n",
        "            in a 2-D numpy array where Pxy[i][j]=P(X=i,Y=j)\n",
        "    Return:\n",
        "    -------\n",
        "    - The joint entropy H(X,Y) as a number (integer, float or double).\n",
        "    \"\"\"\n",
        "    return -np.sum(Pxy*np.log2(Pxy), where=(Pxy > 0)) # H(X,Y) = -sum(P(Xi,Yj) * log2(P(Xi,Yj)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "gxqVI5qt14bS"
      },
      "source": [
        "### Question 3\n",
        "\n",
        "Write a function conditional_entropy that computes the conditional entropy $\\mathcal{H(X|Y)}$ of a discrete random variable $\\mathcal{X}$ given another discrete random variable $\\mathcal{Y}$ from the joint probability distribution $P_\\mathcal{X,Y}$. Give the mathematical formula that you are using and explain the key parts of your implementation. Describe an equivalent way of computing that quantity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "P-HBnYxh14bS"
      },
      "outputs": [],
      "source": [
        "def conditional_entropy(Pxy):\n",
        "    \"\"\"\n",
        "    Computes the conditional entropy from the joint probability distribution.\n",
        "    Arguments:\n",
        "    ----------\n",
        "    - Pxy:  joint probability distribution of X and Y\n",
        "            in a 2-D numpy array where Pxy[i][j]=P(X=i,Y=j)\n",
        "    Return:\n",
        "    -------\n",
        "    - The conditional entropy H(X|Y) as a number (integer, float or double)\n",
        "    \"\"\"\n",
        "    # Py = np.sum(Pxy, axis=0)\n",
        "    # Hxy = joint_entropy(Pxy)\n",
        "    # Hy = entropy(Py)\n",
        "    # Hx_y = Hxy - Hy # H(X|Y) = H(X,Y) - H(Y)\n",
        "    # return Hx_y\n",
        "\n",
        "    return joint_entropy(Pxy) - entropy(np.sum(Pxy, axis=0)) # H(X|Y) = H(X,Y) - H(Y)\n",
        "    # Alternative: return -np.sum(Pxy * np.log2(Pxy / np.sum(Pxy, axis=0), where=(Pxy > 0)), where=(Pxy > 0)) # H(X|Y) = -sum(P(Xi,Yj) * log2(P(Xi,Yj)/P(Yj))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "t5a3I5RV14bT"
      },
      "source": [
        "### Question 4\n",
        "\n",
        "Write a function mutual_information that computes the mutual information $\\mathcal{I(X;Y)}$ between two discrete random variables $\\mathcal{X}$ and $\\mathcal{Y}$ from the joint probability distribution $P_\\mathcal{X,Y}$ . Give the mathematical formula that you are using and explain the key parts of your implementation. What can you deduce from the mutual information $\\mathcal{I(X;Y)}$ on the relationship between $\\mathcal{X}$ and $\\mathcal{Y}$? Discuss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1-97kFCi14bT"
      },
      "outputs": [],
      "source": [
        "def mutual_information(Pxy):\n",
        "    \"\"\"\n",
        "    Computes the mutual information I(X;Y) from joint probability distribution\n",
        "\n",
        "    Arguments:\n",
        "    ----------\n",
        "    - Pxy:  joint probability distribution of X and Y\n",
        "            in a 2-D numpy array where Pxy[i][j]=P(X=i,Y=j)\n",
        "    Return:\n",
        "    -------\n",
        "    - The mutual information I(X;Y) as a number (integer, float or double)\n",
        "    \"\"\"\n",
        "    # Px = np.sum(Pxy, axis=1)\n",
        "    # Hx = entropy(Px)\n",
        "    # Hx_y = conditional_entropy(Pxy)\n",
        "    # Ixy = Hx - Hx_y # I(X;Y) = H(X) - H(X|Y)\n",
        "    # return Ixy\n",
        "\n",
        "    return entropy(np.sum(Pxy, axis=1)) - conditional_entropy(Pxy) # I(X;Y) = H(X) - H(X|Y)\n",
        "    # Alternative: return entropy(np.sum(Pxy, axis=1)) + entropy(np.sum(Pxy, axis=0)) - joint_entropy(Pxy) # I(X;Y) = H(X) + H(Y) - H(X,Y)\n",
        "    # Alternative: return np.sum(Pxy * np.log2(Pxy / np.outer(np.sum(Pxy, axis=1), np.sum(Pxy, axis=0))), where=(Pxy > 0)) # I(X;Y) = +sum(P(Xi,Yj) * log2(P(Xi,Yj)/(P(Xi)*P(Yj)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "HLC7Qkyk14bU"
      },
      "source": [
        "### Question 5\n",
        "\n",
        "Let $\\mathcal{X}$, $\\mathcal{Y}$ and $\\mathcal{Z}$ be three discrete random variables. Write the functions cond_joint_entropy and cond_mutual_information that respectively compute $\\mathcal{H(X,Y|Z)}$ and $\\mathcal{I(X;Y|Z)}$ of two discrete random variable $\\mathcal{X}$, $\\mathcal{Y}$ given another discrete random variable $\\mathcal{Z}$ from their joint probability distribution $P_\\mathcal{X,Y,Z}$. Give the mathematical formulas that you are using and explain the key parts of your implementation.\n",
        "Suggestion: Observe the mathematical definitions of these quantities and think how you could derive them from the joint entropy and the mutual information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IeWejuNa14bU"
      },
      "outputs": [],
      "source": [
        "def cond_joint_entropy(Pxyz):\n",
        "    \"\"\"\n",
        "    Computes the conditional joint entropy of X, Y knowing Z\n",
        "    from the joint probability distribution Pxyz\n",
        "    Arguments:\n",
        "    ----------\n",
        "    - Pxyz: joint probability distribution of X, Y and Z\n",
        "            in a 3-D array where Pxyz[i][j][k]=P(X=i,Y=j,Z=k)\n",
        "    Return:\n",
        "    -------\n",
        "    - The conditional joint entropy H(X,Y|Z) as a number (integer, float or double)\n",
        "\n",
        "    \"\"\"\n",
        "    return -np.sum(Pxyz * np.log2(Pxyz / np.sum(Pxyz, axis=(0,1), keepdims=True)), where=(Pxyz > 0)) # H(X,Y|Z) = -sum(P(Xi,Yj,Zk) * log2(P(Xi,Yj,Zk)/(P(Zk)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xPKeiYmc14bV"
      },
      "outputs": [],
      "source": [
        "def cond_mutual_information(Pxyz):\n",
        "    \"\"\"\n",
        "    Computes the conditional mutual information of X, Y knowing Z\n",
        "    from joint probability distribution Pxyz\n",
        "    Arguments:\n",
        "    ----------\n",
        "    - Pxyz: joint probability distribution of X, Y and Z\n",
        "            in a 3-D array where Pxyz[i][j][k]=P(X=i,Y=j,Z=k)\n",
        "    Return:\n",
        "    -------\n",
        "    - I(X;Y|Z): The conditional joint entropy as a number (integer, float or double)\n",
        "\n",
        "    \"\"\"\n",
        "    # Pxz = np.sum(Pxyz, axis=1)\n",
        "    # Pyz = np.sum(Pxyz, axis=0)\n",
        "    # Hx_z = conditional_entropy(Pxz)\n",
        "    # Hy_z = conditional_entropy(Pyz)\n",
        "    # Hxy_z = cond_joint_entropy(Pxyz)\n",
        "    # Ixy_z = Hx_z + Hy_z - Hxy_z # I(X;Y|Z) = H(X|Z) + H(Y|Z) - H(X,Y|Z)\n",
        "    # return Ixy_z\n",
        "\n",
        "    return conditional_entropy(np.sum(Pxyz, axis=1)) + conditional_entropy(np.sum(Pxyz, axis=0)) - cond_joint_entropy(Pxyz) # I(X;Y|Z) = H(X|Z) + H(Y|Z) - H(X,Y|Z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "5TfG2aWt14bV"
      },
      "outputs": [],
      "source": [
        "# [Locked Cell] Evaluation of your functions by the examiner.\n",
        "# You don't have access to the evaluation, this will be done by the examiner.\n",
        "# Therefore, this cell will return nothing for the students.\n",
        "import os\n",
        "if os.path.isfile(\"private_evaluation.py\"):\n",
        "    from private_evaluation import unit_tests\n",
        "    unit_tests(entropy, joint_entropy, conditional_entropy, mutual_information, cond_joint_entropy, cond_mutual_information)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "4MSJsvry14bV"
      },
      "source": [
        "### Predicting the result of the Information and Coding Theory exam\n",
        "\n",
        "You may create cells below to answer the different questions related to result of the Information and Coding Theory exam questions. Unlike in the first part (Implementation), you are free to define as many cells as you need below to answer the different questions. Try to be structured and clear in your code (comment it if necessary). Note that you have to answer the questions in the pdf report, including the numbers you get!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfXrBeukaUe-"
      },
      "source": [
        "### Loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "eKpxn412admp",
        "outputId": "2905f3cf-049a-4708-8581-b91855789856"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5000 entries, 0 to 4999\n",
            "Data columns (total 11 columns):\n",
            " #   Column                            Non-Null Count  Dtype \n",
            "---  ------                            --------------  ----- \n",
            " 0   Exam result                       5000 non-null   object\n",
            " 1   Grade for the probability class   5000 non-null   object\n",
            " 2   Project grade                     5000 non-null   object\n",
            " 3   Time spent on project             5000 non-null   object\n",
            " 4   Time spent studying               5000 non-null   object\n",
            " 5   Interest in the course            5000 non-null   object\n",
            " 6   Weather the week before the exam  5000 non-null   object\n",
            " 7   Date                              5000 non-null   object\n",
            " 8   Location                          5000 non-null   object\n",
            " 9   Master                            5000 non-null   object\n",
            " 10  Evalens score of the course       5000 non-null   object\n",
            "dtypes: object(11)\n",
            "memory usage: 429.8+ KB\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "file_path = \"data.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display basic information about the dataset\n",
        "df.info()\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KteNw4Bah_PK"
      },
      "source": [
        "###  Joint probability function & Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NFFZ44nRamNU"
      },
      "outputs": [],
      "source": [
        "def probability(data, var_lst=[]):\n",
        "    \"\"\"\n",
        "    Calculate the marginal probability or joint probability of given variables in the dataframe.\n",
        "\n",
        "    Parameters:\n",
        "    data (pd.DataFrame): The dataframe containing the data.\n",
        "    variables (str): The columns for which to calculate the joint probability.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A dataframe containing the joint probability of the given variables.\n",
        "    \"\"\"\n",
        "    # var_lst = list(reversed(var_lst))\n",
        "\n",
        "    if len(var_lst) == 0:\n",
        "        return None\n",
        "\n",
        "    # Compute frequency table\n",
        "    joint_df = data.groupby(var_lst).size().reset_index(name='count')\n",
        "\n",
        "    # Get unique values for each variable\n",
        "    unique_values = [data[var].unique() for var in var_lst]\n",
        "\n",
        "    # Create full probability table with all combinations\n",
        "    if len(var_lst) == 1:\n",
        "        full_joint_df = joint_df\n",
        "        full_joint_df['proba'] = full_joint_df['count'] / full_joint_df['count'].sum()\n",
        "        probability_array = full_joint_df['proba'].values\n",
        "    else:\n",
        "        full_index = pd.MultiIndex.from_product(unique_values, names=var_lst)\n",
        "        full_joint_df = joint_df.set_index(var_lst).reindex(full_index, fill_value=0).reset_index()\n",
        "        full_joint_df['proba'] = full_joint_df['count'] / full_joint_df['count'].sum()\n",
        "        probability_array = full_joint_df['proba'].values.reshape([len(vals) for vals in unique_values])\n",
        "\n",
        "    return probability_array\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmAGqxOKahhy"
      },
      "source": [
        "### Question 6\n",
        "\n",
        "Compute and report the entropy of each variable, and compare each value with its corresponding variable cardinality. What do you notice? Justify theoretically.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "79VqolNjavh6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Entropy, H(X)</th>\n",
              "      <th>Cardinality, |X|</th>\n",
              "      <th>Probability distribution, P(X)</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Variable, X</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Weather the week before the exam</th>\n",
              "      <td>0.980511</td>\n",
              "      <td>2</td>\n",
              "      <td>[0.418, 0.582]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Exam result</th>\n",
              "      <td>0.991254</td>\n",
              "      <td>2</td>\n",
              "      <td>[0.555, 0.445]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Master</th>\n",
              "      <td>1.481246</td>\n",
              "      <td>3</td>\n",
              "      <td>[0.5028, 0.3008, 0.1964]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Grade for the probability class</th>\n",
              "      <td>1.489325</td>\n",
              "      <td>3</td>\n",
              "      <td>[0.2002, 0.4948, 0.305]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Interest in the course</th>\n",
              "      <td>1.513900</td>\n",
              "      <td>3</td>\n",
              "      <td>[0.4106, 0.397, 0.1924]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Evalens score of the course</th>\n",
              "      <td>1.515841</td>\n",
              "      <td>3</td>\n",
              "      <td>[0.2128, 0.3234, 0.4638]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <td>1.584271</td>\n",
              "      <td>3</td>\n",
              "      <td>[0.332, 0.3466, 0.3214]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Location</th>\n",
              "      <td>1.584961</td>\n",
              "      <td>3</td>\n",
              "      <td>[0.3338, 0.3326, 0.3336]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Project grade</th>\n",
              "      <td>1.493609</td>\n",
              "      <td>4</td>\n",
              "      <td>[0.2242, 0.5178, 0.2564, 0.0016]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time spent studying</th>\n",
              "      <td>1.621822</td>\n",
              "      <td>5</td>\n",
              "      <td>[0.5886, 0.0654, 0.2514, 0.0378, 0.0568]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time spent on project</th>\n",
              "      <td>1.978461</td>\n",
              "      <td>5</td>\n",
              "      <td>[0.477, 0.0554, 0.2048, 0.1354, 0.1274]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  Entropy, H(X)  Cardinality, |X|  \\\n",
              "Variable, X                                                         \n",
              "Weather the week before the exam       0.980511                 2   \n",
              "Exam result                            0.991254                 2   \n",
              "Master                                 1.481246                 3   \n",
              "Grade for the probability class        1.489325                 3   \n",
              "Interest in the course                 1.513900                 3   \n",
              "Evalens score of the course            1.515841                 3   \n",
              "Date                                   1.584271                 3   \n",
              "Location                               1.584961                 3   \n",
              "Project grade                          1.493609                 4   \n",
              "Time spent studying                    1.621822                 5   \n",
              "Time spent on project                  1.978461                 5   \n",
              "\n",
              "                                            Probability distribution, P(X)  \n",
              "Variable, X                                                                 \n",
              "Weather the week before the exam                            [0.418, 0.582]  \n",
              "Exam result                                                 [0.555, 0.445]  \n",
              "Master                                            [0.5028, 0.3008, 0.1964]  \n",
              "Grade for the probability class                    [0.2002, 0.4948, 0.305]  \n",
              "Interest in the course                             [0.4106, 0.397, 0.1924]  \n",
              "Evalens score of the course                       [0.2128, 0.3234, 0.4638]  \n",
              "Date                                               [0.332, 0.3466, 0.3214]  \n",
              "Location                                          [0.3338, 0.3326, 0.3336]  \n",
              "Project grade                             [0.2242, 0.5178, 0.2564, 0.0016]  \n",
              "Time spent studying               [0.5886, 0.0654, 0.2514, 0.0378, 0.0568]  \n",
              "Time spent on project              [0.477, 0.0554, 0.2048, 0.1354, 0.1274]  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Compute the entropy of each variable\n",
        "entropy_results = {}\n",
        "\n",
        "# Denote X as each variable in the dataset\n",
        "\n",
        "for col in df.columns:\n",
        "    X_proba = probability(df, [col])\n",
        "    entropy_value = entropy(X_proba)\n",
        "    cardinality = len(X_proba)\n",
        "    entropy_results[col] = {\"Entropy, H(X)\": entropy_value, \"Cardinality, |X|\": cardinality, \"Probability distribution, P(X)\": X_proba}\n",
        "\n",
        "# Convert results into a DataFrame\n",
        "entropy_df = pd.DataFrame.from_dict(entropy_results, orient=\"index\")\n",
        "entropy_df.index.name = \"Variable, X\"\n",
        "\n",
        "# Display the results\n",
        "display(entropy_df.sort_values([\"Cardinality, |X|\", \"Entropy, H(X)\"], ascending=[True, True]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al0ZysBVa1We"
      },
      "source": [
        "### Question 7\n",
        "\n",
        "Compute and report the conditional entropy of Exam result given each of the other variables. Considering the variable descriptions, what do you notice when the conditioning variable is (a) Interest in the course and (b) master?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vnCApMj8a0mK"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Conditional entropy, H(Exam result | Y)</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Variable, Y</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Time spent studying</th>\n",
              "      <td>0.865424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Interest in the course</th>\n",
              "      <td>0.910074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time spent on project</th>\n",
              "      <td>0.914489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Project grade</th>\n",
              "      <td>0.917995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Evalens score of the course</th>\n",
              "      <td>0.920023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <td>0.970676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Grade for the probability class</th>\n",
              "      <td>0.980955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Weather the week before the exam</th>\n",
              "      <td>0.989805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Master</th>\n",
              "      <td>0.991109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Location</th>\n",
              "      <td>0.991156</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  Conditional entropy, H(Exam result | Y)\n",
              "Variable, Y                                                              \n",
              "Time spent studying                                              0.865424\n",
              "Interest in the course                                           0.910074\n",
              "Time spent on project                                            0.914489\n",
              "Project grade                                                    0.917995\n",
              "Evalens score of the course                                      0.920023\n",
              "Date                                                             0.970676\n",
              "Grade for the probability class                                  0.980955\n",
              "Weather the week before the exam                                 0.989805\n",
              "Master                                                           0.991109\n",
              "Location                                                         0.991156"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Compute the conditional entropy of \"Exam result\" given each other variable\n",
        "cond_entropy_results = {}\n",
        "\n",
        "# Denote X as the variable \"Exam result\", Y as the other variables\n",
        "X_var = \"Exam result\"\n",
        "\n",
        "# Compute the conditional entropy H(X|Y) for each variable Y\n",
        "for col in df.columns:\n",
        "    if col != X_var:\n",
        "        joint_proba = probability(df, [X_var, col])\n",
        "        cond_entropy_value = conditional_entropy(joint_proba)\n",
        "        cond_entropy_results[col] = {\"Conditional entropy, H(Exam result | Y)\": cond_entropy_value}\n",
        "\n",
        "# Convert results into a DataFrame\n",
        "cond_entropy_df = pd.DataFrame.from_dict(cond_entropy_results, orient=\"index\")\n",
        "cond_entropy_df.index.name = \"Variable, Y\"\n",
        "\n",
        "# Display results\n",
        "display(cond_entropy_df.sort_values(\"Conditional entropy, H(Exam result | Y)\", ascending=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJqp0B8_a4vn"
      },
      "source": [
        "### Question 8\n",
        "\n",
        "Compute the mutual information between the variables location and Evalens score of the course. What can you deduce about the relationship between these two variables?  What about the variables Time spent on the project and project grade?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uZcw8oH2a5S_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\dinhd\\AppData\\Local\\Temp\\ipykernel_17160\\1088038093.py:12: RuntimeWarning: divide by zero encountered in log2\n",
            "  return -np.sum(Pxy*np.log2(Pxy), where=(Pxy > 0)) # H(X,Y) = -sum(P(Xi,Yj) * log2(P(Xi,Yj)))\n",
            "C:\\Users\\dinhd\\AppData\\Local\\Temp\\ipykernel_17160\\1088038093.py:12: RuntimeWarning: invalid value encountered in multiply\n",
            "  return -np.sum(Pxy*np.log2(Pxy), where=(Pxy > 0)) # H(X,Y) = -sum(P(Xi,Yj) * log2(P(Xi,Yj)))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Mutual information, I(X; Y)</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(Variable X; Variable Y)</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>(Location; Evalens)</th>\n",
              "      <td>0.000199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(Time spent on project; Project grade)</th>\n",
              "      <td>0.685335</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        Mutual information, I(X; Y)\n",
              "(Variable X; Variable Y)                                           \n",
              "(Location; Evalens)                                        0.000199\n",
              "(Time spent on project; Project grade)                     0.685335"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Compute mutual information of \"Location\" and \"Evalens score of the course\"\n",
        "joint_proba_location_evalens = probability(df, ['Location', 'Evalens score of the course'])\n",
        "mutual_info_location_evalens = mutual_information(joint_proba_location_evalens)\n",
        "\n",
        "# Compute mutual information of \"Time spent on project\" and \"Project grade\"\n",
        "joint_proba_time_project_grade = probability(df, ['Time spent on project', 'Project grade'])\n",
        "mutual_info_time_project_grade = mutual_information(joint_proba_time_project_grade)\n",
        "\n",
        "mutual_info_results = {\n",
        "    \"(Location; Evalens)\": mutual_info_location_evalens,\n",
        "    \"(Time spent on project; Project grade)\": mutual_info_time_project_grade\n",
        "}\n",
        "\n",
        "# Convert results into a DataFrame\n",
        "mutual_info_df = pd.DataFrame.from_dict(mutual_info_results, orient=\"index\", columns=[\"Mutual information, I(X; Y)\"])\n",
        "mutual_info_df.index.name = \"(Variable X; Variable Y)\"\n",
        "\n",
        "# Display results\n",
        "display(mutual_info_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcOvjx29a-xA"
      },
      "source": [
        "### Question 9\n",
        "\n",
        "A student in Computer Science from the University of Li√®ge bets his friends that he can predict the upcoming exam by accessing the dataset. However, his hacking skills are still weak. Therefore, he can only access a single variable of the dataset to make its prediction. Using only the mutual information, which variable should he choose to get? Would using conditional entropy lead to another choice?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rnWLgoTpa-k7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Mutual information, I(Exam result; Y)</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Variable, Y</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Time spent studying</th>\n",
              "      <td>0.125830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Interest in the course</th>\n",
              "      <td>0.081181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time spent on project</th>\n",
              "      <td>0.076765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Project grade</th>\n",
              "      <td>0.073259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Evalens score of the course</th>\n",
              "      <td>0.071231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <td>0.020578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Grade for the probability class</th>\n",
              "      <td>0.010299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Weather the week before the exam</th>\n",
              "      <td>0.001449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Master</th>\n",
              "      <td>0.000145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Location</th>\n",
              "      <td>0.000098</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  Mutual information, I(Exam result; Y)\n",
              "Variable, Y                                                            \n",
              "Time spent studying                                            0.125830\n",
              "Interest in the course                                         0.081181\n",
              "Time spent on project                                          0.076765\n",
              "Project grade                                                  0.073259\n",
              "Evalens score of the course                                    0.071231\n",
              "Date                                                           0.020578\n",
              "Grade for the probability class                                0.010299\n",
              "Weather the week before the exam                               0.001449\n",
              "Master                                                         0.000145\n",
              "Location                                                       0.000098"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Compute the mutual information of \"Exam result\" and each of the other variables\n",
        "mutual_info_results = {}\n",
        "\n",
        "# Denote X as the variable \"Exam result\", Y as the other variables\n",
        "X_var = \"Exam result\"\n",
        "\n",
        "# Compute the mutual information I(X;Y) for each variable Y\n",
        "for col in df.columns:\n",
        "    if col != X_var:\n",
        "        joint_proba = probability(df, [X_var, col])\n",
        "        mutual_info = mutual_information(joint_proba)\n",
        "        mutual_info_results[col] = mutual_info\n",
        "\n",
        "# Convert results into a DataFrame\n",
        "mutual_info_df = pd.DataFrame.from_dict(mutual_info_results, orient=\"index\", columns=[\"Mutual information, I(Exam result; Y)\"])\n",
        "mutual_info_df.index.name = \"Variable, Y\"\n",
        "\n",
        "# Display results\n",
        "display(mutual_info_df.sort_values(\"Mutual information, I(Exam result; Y)\", ascending=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjmjsF5gbBOD"
      },
      "source": [
        "### Question 10\n",
        "\n",
        "With the interest in the course considered as known, would you change your answer from the previous question? What can you say about the amount of information provided by this variable? Compare this value with previous results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-ryoDr0ebCDw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\dinhd\\AppData\\Local\\Temp\\ipykernel_17160\\1088038093.py:12: RuntimeWarning: divide by zero encountered in log2\n",
            "  return -np.sum(Pxy*np.log2(Pxy), where=(Pxy > 0)) # H(X,Y) = -sum(P(Xi,Yj) * log2(P(Xi,Yj)))\n",
            "C:\\Users\\dinhd\\AppData\\Local\\Temp\\ipykernel_17160\\1088038093.py:12: RuntimeWarning: invalid value encountered in multiply\n",
            "  return -np.sum(Pxy*np.log2(Pxy), where=(Pxy > 0)) # H(X,Y) = -sum(P(Xi,Yj) * log2(P(Xi,Yj)))\n",
            "C:\\Users\\dinhd\\AppData\\Local\\Temp\\ipykernel_17160\\1390812537.py:14: RuntimeWarning: divide by zero encountered in log2\n",
            "  return -np.sum(Pxyz * np.log2(Pxyz / np.sum(Pxyz, axis=(0,1), keepdims=True)), where=(Pxyz > 0)) # H(X,Y|Z) = -sum(P(Xi,Yj,Zk) * log2(P(Xi,Yj,Zk)/(P(Zk)))\n",
            "C:\\Users\\dinhd\\AppData\\Local\\Temp\\ipykernel_17160\\1390812537.py:14: RuntimeWarning: invalid value encountered in multiply\n",
            "  return -np.sum(Pxyz * np.log2(Pxyz / np.sum(Pxyz, axis=(0,1), keepdims=True)), where=(Pxyz > 0)) # H(X,Y|Z) = -sum(P(Xi,Yj,Zk) * log2(P(Xi,Yj,Zk)/(P(Zk)))\n",
            "C:\\Users\\dinhd\\AppData\\Local\\Temp\\ipykernel_17160\\1088038093.py:12: RuntimeWarning: divide by zero encountered in log2\n",
            "  return -np.sum(Pxy*np.log2(Pxy), where=(Pxy > 0)) # H(X,Y) = -sum(P(Xi,Yj) * log2(P(Xi,Yj)))\n",
            "C:\\Users\\dinhd\\AppData\\Local\\Temp\\ipykernel_17160\\1088038093.py:12: RuntimeWarning: invalid value encountered in multiply\n",
            "  return -np.sum(Pxy*np.log2(Pxy), where=(Pxy > 0)) # H(X,Y) = -sum(P(Xi,Yj) * log2(P(Xi,Yj)))\n",
            "C:\\Users\\dinhd\\AppData\\Local\\Temp\\ipykernel_17160\\1390812537.py:14: RuntimeWarning: divide by zero encountered in log2\n",
            "  return -np.sum(Pxyz * np.log2(Pxyz / np.sum(Pxyz, axis=(0,1), keepdims=True)), where=(Pxyz > 0)) # H(X,Y|Z) = -sum(P(Xi,Yj,Zk) * log2(P(Xi,Yj,Zk)/(P(Zk)))\n",
            "C:\\Users\\dinhd\\AppData\\Local\\Temp\\ipykernel_17160\\1390812537.py:14: RuntimeWarning: invalid value encountered in multiply\n",
            "  return -np.sum(Pxyz * np.log2(Pxyz / np.sum(Pxyz, axis=(0,1), keepdims=True)), where=(Pxyz > 0)) # H(X,Y|Z) = -sum(P(Xi,Yj,Zk) * log2(P(Xi,Yj,Zk)/(P(Zk)))\n",
            "C:\\Users\\dinhd\\AppData\\Local\\Temp\\ipykernel_17160\\1088038093.py:12: RuntimeWarning: divide by zero encountered in log2\n",
            "  return -np.sum(Pxy*np.log2(Pxy), where=(Pxy > 0)) # H(X,Y) = -sum(P(Xi,Yj) * log2(P(Xi,Yj)))\n",
            "C:\\Users\\dinhd\\AppData\\Local\\Temp\\ipykernel_17160\\1088038093.py:12: RuntimeWarning: invalid value encountered in multiply\n",
            "  return -np.sum(Pxy*np.log2(Pxy), where=(Pxy > 0)) # H(X,Y) = -sum(P(Xi,Yj) * log2(P(Xi,Yj)))\n",
            "C:\\Users\\dinhd\\AppData\\Local\\Temp\\ipykernel_17160\\1390812537.py:14: RuntimeWarning: divide by zero encountered in log2\n",
            "  return -np.sum(Pxyz * np.log2(Pxyz / np.sum(Pxyz, axis=(0,1), keepdims=True)), where=(Pxyz > 0)) # H(X,Y|Z) = -sum(P(Xi,Yj,Zk) * log2(P(Xi,Yj,Zk)/(P(Zk)))\n",
            "C:\\Users\\dinhd\\AppData\\Local\\Temp\\ipykernel_17160\\1390812537.py:14: RuntimeWarning: invalid value encountered in multiply\n",
            "  return -np.sum(Pxyz * np.log2(Pxyz / np.sum(Pxyz, axis=(0,1), keepdims=True)), where=(Pxyz > 0)) # H(X,Y|Z) = -sum(P(Xi,Yj,Zk) * log2(P(Xi,Yj,Zk)/(P(Zk)))\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>I(Exam result; Y | Interest in the course)</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Variable, Y</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Time spent studying</th>\n",
              "      <td>0.045481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Evalens score of the course</th>\n",
              "      <td>0.035664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <td>0.029738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Grade for the probability class</th>\n",
              "      <td>0.010938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Project grade</th>\n",
              "      <td>0.009282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Weather the week before the exam</th>\n",
              "      <td>0.002484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Location</th>\n",
              "      <td>0.002438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Time spent on project</th>\n",
              "      <td>0.001196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Master</th>\n",
              "      <td>0.000784</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  I(Exam result; Y | Interest in the course)\n",
              "Variable, Y                                                                 \n",
              "Time spent studying                                                 0.045481\n",
              "Evalens score of the course                                         0.035664\n",
              "Date                                                                0.029738\n",
              "Grade for the probability class                                     0.010938\n",
              "Project grade                                                       0.009282\n",
              "Weather the week before the exam                                    0.002484\n",
              "Location                                                            0.002438\n",
              "Time spent on project                                               0.001196\n",
              "Master                                                              0.000784"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Compute the conditional joint entropy of \"Exam result\" and each of the other variables when knowing \"Interest in the course\"\n",
        "q10_results = {}\n",
        "\n",
        "# Denote X as the variable \"Exam result\", Y as the other variables, Z as the variable \"Interest in the course\"\n",
        "X_var = \"Exam result\"\n",
        "Z_var = \"Interest in the course\"\n",
        "\n",
        "# Compute the conditional joint entropy H(X,Y|Z) and mutual information I(X;Y|Z) for each variable Y\n",
        "for col in df.columns:\n",
        "    if col != X_var and col != Z_var:\n",
        "        joint_proba = probability(df, [X_var, col, Z_var])\n",
        "        cond_mutual_info = cond_mutual_information(joint_proba)\n",
        "        q10_results[col] = {\"I(Exam result; Y | Interest in the course)\": cond_mutual_info}\n",
        "\n",
        "# Convert results into a DataFrame\n",
        "cond_joint_entropy_df = pd.DataFrame.from_dict(q10_results, orient=\"index\")\n",
        "cond_joint_entropy_df.index.name = \"Variable, Y\"\n",
        "\n",
        "# Display results\n",
        "display(cond_joint_entropy_df.sort_values(\"I(Exam result; Y | Interest in the course)\", ascending=False))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
